{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afd8aa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9fb86a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c77025",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from typing import List, Dict, Iterator, Tuple\n",
    "\n",
    "BASE_DIR = \"D:\\zman\\graph\\data\" # update to your parquet root\n",
    "MAP_DIR = \"./maps\"\n",
    "EDGE_DIR = \"./edges\"\n",
    "FEATURE_DIR = \"./features\"\n",
    "\n",
    "os.makedirs(MAP_DIR, exist_ok=True)\n",
    "os.makedirs(EDGE_DIR, exist_ok=True)\n",
    "os.makedirs(FEATURE_DIR, exist_ok=True)\n",
    "\n",
    "# Example table folders (update names to your actual folder names)\n",
    "TABLE_PATHS = {\n",
    "'node_pekerja': os.path.join(BASE_DIR, 'node_pekerja'),\n",
    "'node_nasabah': os.path.join(BASE_DIR, 'node_nasabah'),\n",
    "'node_simpanan': os.path.join(BASE_DIR, 'node_simpanan'),\n",
    "'node_pinjaman': os.path.join(BASE_DIR, 'node_pinjaman'),\n",
    "'node_transaksi': os.path.join(BASE_DIR, 'node_transaksi'),\n",
    "'edge_rek_debit': os.path.join(BASE_DIR, 'edge_rek_debit'),\n",
    "'edge_rek_credit': os.path.join(BASE_DIR, 'edge_rek_credit'),\n",
    "'edge_nasabah_memiliki_simp': os.path.join(BASE_DIR, 'edge_nasabah_memiliki_simp'),\n",
    "'edge_nasabah_memiliki_pinj': os.path.join(BASE_DIR, 'edge_nasabah_memiliki_pinj'),\n",
    "'edge_nasabah_is_pekerja': os.path.join(BASE_DIR, 'edge_nasabah_is_pekerja'),\n",
    "}\n",
    "\n",
    "# Parquet partition column name for month\n",
    "PARTITION_COL = 'period'  # e.g. period=202407\n",
    "\n",
    "# Recommended runtime knobs for T1000 4GB GPU + 32GB RAM\n",
    "KNOBS = {\n",
    "    'device': \"mps\",\n",
    "    'hidden_dim': 32,\n",
    "    'final_feat_dim': 64,\n",
    "    'batch_size_nodes': 256,\n",
    "    'num_neighbors': [12, 6],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38013b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\zman\\graph\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Dependencies\n",
    "# -----------------------------------------------------------------------------\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import lmdb\n",
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "from torch_geometric.nn import HeteroConv, SAGEConv\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.amp import GradScaler\n",
    "from sklearn.metrics import precision_recall_curve, roc_auc_score\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1) Streaming helper: pyarrow Dataset -> pandas batches\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def stream_table(path: str, columns: List[str] = None, month: str = None, batch_size: int = 75_000) -> Iterator[pd.DataFrame]:\n",
    "    \"\"\"Yield pandas DataFrames in small batches using pyarrow.dataset.\n",
    "    - path: folder root of a parquet table (partitioned by hive style)\n",
    "    - columns: list of columns to read\n",
    "    - month: partition filter value (e.g. '202407')\n",
    "    - batch_size: rows per batch\n",
    "    \"\"\"\n",
    "    ds_obj = ds.dataset(path, format='parquet', partitioning='hive')\n",
    "    flt = None\n",
    "    if month is not None:\n",
    "        flt = ds.field(PARTITION_COL) == month\n",
    "    for rb in ds_obj.to_batches(columns=columns, filter=flt, batch_size=batch_size):\n",
    "        yield rb.to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de4741bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_obj = ds.dataset(\"D:/zman/graph/data/node_pekerja\", format='parquet', partitioning='hive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5eca529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62764"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_obj.count_rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9ef8980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 2) ID mapping (pickle-backed). Use LMDB alternative for very large maps.\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def build_or_load_map(values_iter: Iterator[pd.DataFrame], key_col: str, map_path: str) -> Dict:\n",
    "    \"\"\"Build mapping from original key to contiguous int IDs (0..N-1).\n",
    "    If map_path exists, load and return it.\n",
    "    \"\"\"\n",
    "    if os.path.exists(map_path):\n",
    "        with open(map_path, 'rb') as f:\n",
    "            print(f\"Loading map from {map_path}\")\n",
    "            return pickle.load(f)\n",
    "\n",
    "    mapping = {}\n",
    "    nid = 0\n",
    "    for df in values_iter:\n",
    "        vals = pd.Series(df[key_col].dropna().unique())\n",
    "        for v in vals:\n",
    "            if v not in mapping:\n",
    "                mapping[v] = nid\n",
    "                nid += 1\n",
    "    with open(map_path, 'wb') as f:\n",
    "        pickle.dump(mapping, f)\n",
    "    print(f\"Saved map {map_path} with {nid} entries\")\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def build_or_load_lmdb_map(values_iter: Iterator[pd.DataFrame], key_col: str, lmdb_path: str) -> None:\n",
    "    \"\"\"Alternative using LMDB: insert keys and their IDs. This writes to disk as LMDB DB.\n",
    "    lmdb_path is a file path for the LMDB environment.\n",
    "    \"\"\"\n",
    "    if os.path.exists(lmdb_path):\n",
    "        print(f\"LMDB exists: {lmdb_path}\")\n",
    "        return\n",
    "    env = lmdb.open(lmdb_path, map_size=1024**4)\n",
    "    nid = 0\n",
    "    with env.begin(write=True) as txn:\n",
    "        for df in values_iter:\n",
    "            for v in df[key_col].dropna().unique():\n",
    "                k = str(v).encode()\n",
    "                if txn.get(k) is None:\n",
    "                    txn.put(k, str(nid).encode())\n",
    "                    nid += 1\n",
    "    env.close()\n",
    "    print(f\"Built LMDB map {lmdb_path} entries={nid}\")\n",
    "\n",
    "# helper load pickle\n",
    "\n",
    "def load_pickle_map(map_path: str) -> Dict:\n",
    "    with open(map_path, 'rb') as f:\n",
    "        return pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cc1f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 3) Edge builder: stream edges and write mapped integer edge parquet\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def build_edges_from_parquet(edge_table_path: str, src_col: str, dst_col: str,\n",
    "                             src_map: Dict, dst_map: Dict,\n",
    "                             out_parquet: str, month: str = None,\n",
    "                             cols_to_keep: List[str] = None):\n",
    "    \"\"\"Stream edge table, map src/dst values to integer node IDs and write parquet.\n",
    "    Writes single output parquet file (appends in streaming manner).\n",
    "    \"\"\"\n",
    "    writer = None\n",
    "    written = 0\n",
    "    for df in stream_table(edge_table_path, columns=[src_col, dst_col] + (cols_to_keep or []), month=month, batch_size=75_000):\n",
    "        src_mapped = df[src_col].map(src_map).fillna(-1).astype('int32')\n",
    "        dst_mapped = df[dst_col].map(dst_map).fillna(-1).astype('int32')\n",
    "        out_df = pd.DataFrame({'src_nid': src_mapped.values, 'dst_nid': dst_mapped.values})\n",
    "        if cols_to_keep:\n",
    "            for c in cols_to_keep:\n",
    "                out_df[c] = df[c].values\n",
    "        tbl = pa.Table.from_pandas(out_df)\n",
    "        if writer is None:\n",
    "            writer = pq.ParquetWriter(out_parquet, tbl.schema)\n",
    "        writer.write_table(tbl)\n",
    "        written += len(out_df)\n",
    "    if writer is not None:\n",
    "        writer.close()\n",
    "    print(f\"Wrote {written} edges to {out_parquet}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f7f379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 4) Simple streaming feature aggregation per pekerja\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def agg_features_per_pekerja_simple(transaksi_path: str, map_rek: Dict, map_nasabah: Dict,\n",
    "                                    map_pekerja_from_nasabah: Dict, out_parquet: str,\n",
    "                                    months: List[str]):\n",
    "    \"\"\"Aggregate a few simple features per pekerja by streaming transaksi.\n",
    "    This function assumes you have a mapping from nasabah -> pekerja (edge_nasabah_is_pekerja)\n",
    "    represented by map_pekerja_from_nasabah: nasabah_cif -> pekerja_nid.\n",
    "\n",
    "    Output columns: nid (pekerja id), cnt_txn, sum_amt, cnt_dst_accounts, recent_txn_7d (sketch)\n",
    "    \"\"\"\n",
    "    # initialize accumulation dicts\n",
    "    from collections import defaultdict\n",
    "    agg_cnt = defaultdict(int)\n",
    "    agg_sum = defaultdict(float)\n",
    "    agg_dst_set = defaultdict(set)\n",
    "\n",
    "    for month in months:\n",
    "        for df in stream_table(transaksi_path, columns=['id_trx','src','dst','amt','dsctrc','channel','period'], month=month, batch_size=75_000):\n",
    "            # map account -> nasabah if available (this needs precomputed account->nasabah map)\n",
    "            # For blueprint: assume map_rek maps acctno -> nasabah_cif\n",
    "            df['src_nasabah'] = df['src'].map(map_rek).fillna(-1)\n",
    "            df['dst_nasabah'] = df['dst'].map(map_rek).fillna(-1)\n",
    "            # map nasabah -> pekerja (if a nasabah has a pekerja relation)\n",
    "            df['src_pekerja_nid'] = df['src_nasabah'].map(map_pekerja_from_nasabah).fillna(-1).astype('int32')\n",
    "            df['dst_pekerja_nid'] = df['dst_nasabah'].map(map_pekerja_from_nasabah).fillna(-1).astype('int32')\n",
    "\n",
    "            # accumulate for pekerja appearing on either side\n",
    "            for _, row in df.iterrows():\n",
    "                for pk in ['src_pekerja_nid','dst_pekerja_nid']:\n",
    "                    pn = int(row.get(pk, -1))\n",
    "                    if pn < 0:\n",
    "                        continue\n",
    "                    agg_cnt[pn] += 1\n",
    "                    agg_sum[pn] += float(row.get('amt') or 0.0)\n",
    "                    agg_dst_set[pn].add(row.get('dst'))\n",
    "\n",
    "    # convert to DataFrame and write\n",
    "    rows = []\n",
    "    for nid in agg_cnt.keys():\n",
    "        rows.append({'nid': nid, 'cnt_txn': agg_cnt[nid], 'sum_amt': agg_sum[nid], 'cnt_dst_accounts': len(agg_dst_set[nid])})\n",
    "    out_df = pd.DataFrame(rows)\n",
    "    out_df.to_parquet(out_parquet, index=False)\n",
    "    print(f\"Wrote aggregated features per pekerja to {out_parquet}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e049aafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 5) Build HeteroData (memory-aware): load small node features fully, for large node types\n",
    "#    store only num_nodes and rely on edge parquet for sampling\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def build_heterodata(map_files: Dict[str,str], node_feature_files: Dict[str,str], edge_parquet_files: Dict[Tuple[str,str,str],str]) -> HeteroData:\n",
    "    \"\"\"Construct HeteroData. For large node types (e.g. transaksi), do NOT load full x matrix\n",
    "    (unless user prepared memmapped features). Edges are loaded from parquet and set as tensors.\n",
    "    edge_parquet_files keys are tuples (src_ntype, rel, dst_ntype).\n",
    "    \"\"\"\n",
    "    data = HeteroData()\n",
    "\n",
    "    # pekerja features must be small -> load fully\n",
    "    pekerja_path = node_feature_files.get('pekerja')\n",
    "    if pekerja_path is None:\n",
    "        raise ValueError('pekerja feature parquet is required for Version A')\n",
    "    df_pekerja = pd.read_parquet(pekerja_path)\n",
    "    if 'nid' not in df_pekerja.columns:\n",
    "        # assume index-like ordering if not provided\n",
    "        df_pekerja = df_pekerja.reset_index().rename(columns={'index':'nid'})\n",
    "    feat_cols = [c for c in df_pekerja.columns if c not in ('nid','is_fraud')]\n",
    "    x_pekerja = torch.tensor(df_pekerja[feat_cols].fillna(0).values.astype('float32'))\n",
    "    y_pekerja = torch.tensor(df_pekerja['is_fraud'].fillna(0).values.astype('int64'))\n",
    "    data['pekerja'].x = x_pekerja\n",
    "    data['pekerja'].y = y_pekerja\n",
    "    data['pekerja'].num_nodes = x_pekerja.size(0)\n",
    "\n",
    "    # other node types: if features file provided and small, load; else set num_nodes from map\n",
    "    for ntype in ['nasabah','rekening','transaksi']:\n",
    "        path = node_feature_files.get(ntype)\n",
    "        map_path = map_files.get(ntype)\n",
    "        if path and os.path.exists(path):\n",
    "            df = pd.read_parquet(path)\n",
    "            if 'nid' not in df.columns:\n",
    "                df = df.reset_index().rename(columns={'index':'nid'})\n",
    "            cols = [c for c in df.columns if c != 'nid']\n",
    "            x = torch.tensor(df[cols].fillna(0).values.astype('float32'))\n",
    "            data[ntype].x = x\n",
    "            data[ntype].num_nodes = x.size(0)\n",
    "        elif map_path and os.path.exists(map_path):\n",
    "            # load mapping to count nodes\n",
    "            m = load_pickle_map(map_path)\n",
    "            data[ntype].num_nodes = len(m)\n",
    "        else:\n",
    "            data[ntype].num_nodes = 0\n",
    "\n",
    "    # load edges from parquet and set edge_index for each relation\n",
    "    for (src_ntype, rel, dst_ntype), p in edge_parquet_files.items():\n",
    "        if not os.path.exists(p):\n",
    "            continue\n",
    "        tbl = pq.read_table(p)\n",
    "        df = tbl.to_pandas()\n",
    "        # remove invalid -1 mappings\n",
    "        df = df[(df['src_nid'] >= 0) & (df['dst_nid'] >= 0)]\n",
    "        src = torch.tensor(df['src_nid'].values, dtype=torch.long)\n",
    "        dst = torch.tensor(df['dst_nid'].values, dtype=torch.long)\n",
    "        data[(src_ntype, rel, dst_ntype)].edge_index = torch.vstack([src, dst])\n",
    "    print('Built HeteroData: node counts:', {k: getattr(data[k], 'num_nodes', None) for k in data.node_types})\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "124ae6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 6) NeighborLoader with oversampling helper\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def make_neighbor_loader(data: HeteroData, train_idx: torch.Tensor, pos_idx: torch.Tensor,\n",
    "                         batch_size: int = 256, num_neighbors: Dict = None, oversample_ratio: int = 4, shuffle: bool = True):\n",
    "    if num_neighbors is None:\n",
    "        num_neighbors = KNOBS['num_neighbors']\n",
    "    fraud_oversampled = pos_idx.repeat(oversample_ratio)\n",
    "    # combine: keep order stable\n",
    "    nonfraud_mask = ~torch.isin(train_idx, pos_idx)\n",
    "    combined = torch.cat([fraud_oversampled, train_idx[nonfraud_mask]])\n",
    "    loader = NeighborLoader(\n",
    "        data,\n",
    "        num_neighbors=num_neighbors,\n",
    "        input_nodes=('pekerja', combined),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "    )\n",
    "    return loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "389817e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 7) Compact HeteroSAGE model\n",
    "# -----------------------------------------------------------------------------\n",
    "class CompactHeteroSAGE(torch.nn.Module):\n",
    "    def __init__(self, metadata, in_dims: Dict[str,int], hidden: int = 32, num_layers: int = 2):\n",
    "        super().__init__()\n",
    "        self.metadata = metadata\n",
    "        self.hidden = hidden\n",
    "        self.num_layers = num_layers\n",
    "        self.lin = torch.nn.ModuleDict()\n",
    "        for ntype, dim in in_dims.items():\n",
    "            self.lin[ntype] = torch.nn.Linear(dim, hidden)\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv = HeteroConv({\n",
    "                edge_type: SAGEConv((-1,-1), hidden) for edge_type in metadata[1]\n",
    "            }, aggr='mean')\n",
    "            self.convs.append(conv)\n",
    "        self.classifier = torch.nn.Linear(hidden, 1)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x = {k: F.relu(self.lin[k](v)) for k,v in x_dict.items() if k in self.lin}\n",
    "        for conv in self.convs:\n",
    "            x = conv(x, edge_index_dict)\n",
    "            x = {k: F.relu(v) for k,v in x.items()}\n",
    "        out = self.classifier(x['pekerja']).squeeze(-1)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4774fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 8) Training loop (FP16 + grad accum) and evaluation\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def train_model(model: torch.nn.Module, train_loader, val_loader, device: str = KNOBS['device'], epochs: int = 5,\n",
    "                accum_steps: int = 2, lr: float = 1e-3):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "    # compute pos_weight from training labels if available\n",
    "    # Here we assume train_loader yields batches with 'pekerja'.y available\n",
    "    scaler = GradScaler(device=\"cuda\")\n",
    "    # placeholder: will compute pos_weight externally if full dataset labels accessible\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            batch = batch.to(device)\n",
    "            with autocast():\n",
    "                logits = model(batch.x_dict, batch.edge_index_dict)\n",
    "                y = batch['pekerja'].y.to(torch.float32).to(device)\n",
    "                loss = criterion(logits, y) / accum_steps\n",
    "            scaler.scale(loss).backward()\n",
    "            if (i+1) % accum_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "            total_loss += loss.item() * accum_steps\n",
    "        avg_loss = total_loss / (i+1)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} train_loss={avg_loss:.4f}\")\n",
    "        metrics = evaluate_model(model, val_loader, device)\n",
    "        print(f\"Val metrics: {metrics}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model: torch.nn.Module, loader, device: str = KNOBS['device']):\n",
    "    model.eval()\n",
    "    ys, ys_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            logits = model(batch.x_dict, batch.edge_index_dict)\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            ys_pred.append(probs)\n",
    "            ys.append(batch['pekerja'].y.cpu().numpy())\n",
    "    y_pred = np.concatenate(ys_pred)\n",
    "    y_true = np.concatenate(ys)\n",
    "    auc = roc_auc_score(y_true, y_pred) if len(np.unique(y_true)) > 1 else float('nan')\n",
    "    prec, rec, th = precision_recall_curve(y_true, y_pred)\n",
    "    f1 = 2 * prec * rec / (prec + rec + 1e-12)\n",
    "    if len(f1) > 0:\n",
    "        best_idx = np.nanargmax(f1)\n",
    "        best_thr = th[best_idx] if best_idx < len(th) else 0.5\n",
    "    else:\n",
    "        best_thr = 0.5\n",
    "    return {'auc': float(auc), 'best_thr': float(best_thr)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cae731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# 9) High-level driver for Version A: build maps, edges, features, heterodata, loaders\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def version_a_runner(\n",
    "        months_train: List[str],\n",
    "        months_val: List[str],\n",
    "        months_test: List[str],\n",
    "        label_parquet_path: str,\n",
    "        account_to_nasabah_map_parquet: str = None):\n",
    "    \"\"\"End-to-end runner for Version A.\n",
    "    Steps:\n",
    "    1) Build id maps for node types (pekerja, nasabah, rekening, transaksi)\n",
    "    2) Build edges (mapped integer parquet)\n",
    "    3) Build simple aggregated pekerja features\n",
    "    4) Construct HeteroData and NeighborLoader (train/val/test)\n",
    "    5) Train and validate model\n",
    "    \"\"\"\n",
    "    # 1) Build maps\n",
    "    print('Step 1: building id maps (streaming)')\n",
    "    # pekerja map (small) - from node_pekerja table\n",
    "    pekerja_iter = stream_table(TABLE_PATHS['node_pekerja'], columns=['pn'], month=None, batch_size=75_000)\n",
    "    pekerja_map_path = os.path.join(MAP_DIR, 'map_pekerja.pkl')\n",
    "    pekerja_map = build_or_load_map(pekerja_iter, 'pn', pekerja_map_path)\n",
    "\n",
    "    # nasabah map (could be large) - from node_nasabah\n",
    "    nasabah_iter = stream_table(TABLE_PATHS['node_nasabah'], columns=['cif'], month=None, batch_size=75_000)\n",
    "    nasabah_map_path = os.path.join(MAP_DIR, 'map_nasabah.pkl')\n",
    "    nasabah_map = build_or_load_map(nasabah_iter, 'cif', nasabah_map_path)\n",
    "    \n",
    "    # rekening map (combine simpanan + pinjaman acctno)\n",
    "    # streaming accounts from node_simpanan and node_pinjaman\n",
    "    def acct_iter():\n",
    "        for df in stream_table(TABLE_PATHS['node_simpanan'], columns=['acctno'], month=None, batch_size=75_000):\n",
    "            yield df\n",
    "        for df in stream_table(TABLE_PATHS['node_pinjaman'], columns=['acctno'], month=None, batch_size=75_000):\n",
    "            yield df\n",
    "    rekening_map_path = os.path.join(MAP_DIR, 'map_rekening.pkl')\n",
    "    rekening_map = build_or_load_map(acct_iter(), 'acctno', rekening_map_path)\n",
    "\n",
    "    # transaksi map (id_trx) can be large; we will not map all tx if not needed but create mapping for those referenced by edges\n",
    "    txn_iter = stream_table(TABLE_PATHS['node_transaksi'], columns=['id_trx'], month=None, batch_size=75_000)\n",
    "    txn_map_path = os.path.join(MAP_DIR, 'map_transaksi.pkl')\n",
    "    txn_map = build_or_load_map(txn_iter, 'id_trx', txn_map_path)\n",
    "\n",
    "    # 2) Build edge files: map nasabah-pekerja, rekening->nasabah, transaksi->rekening\n",
    "    print('Step 2: building edge parquet files (mapped integer ids)')\n",
    "    # edge_nasabah_is_pekerja: src=cif, dst=pn\n",
    "    out_edge_pekerja = os.path.join(EDGE_DIR, 'edge_nasabah_is_pekerja.parquet')\n",
    "    build_edges_from_parquet(TABLE_PATHS['edge_nasabah_is_pekerja'], src_col='src', dst_col='dst',\n",
    "                             src_map=nasabah_map, dst_map=pekerja_map, out_parquet=out_edge_pekerja, month=None)\n",
    "\n",
    "    # edge_nasabah_memiliki_simp: src=cif, dst=acctno\n",
    "    out_edge_nasabah_simp = os.path.join(EDGE_DIR, 'edge_nasabah_memiliki_simp.parquet')\n",
    "    build_edges_from_parquet(TABLE_PATHS['edge_nasabah_memiliki_simp'], src_col='src', dst_col='dst',\n",
    "                             src_map=nasabah_map, dst_map=rekening_map, out_parquet=out_edge_nasabah_simp, month=None)\n",
    "\n",
    "    # transaksi -> rekening: use edge_rek_debit (src acct) and edge_rek_credit (dst acct)\n",
    "    out_edge_txn_from = os.path.join(EDGE_DIR, 'edge_transaksi_from_rekening.parquet')\n",
    "    build_edges_from_parquet(TABLE_PATHS['edge_rek_debit'], src_col='src', dst_col='dst',\n",
    "                             src_map=rekening_map, dst_map=txn_map, out_parquet=out_edge_txn_from, month=None)\n",
    "    out_edge_txn_to = os.path.join(EDGE_DIR, 'edge_transaksi_to_rekening.parquet')\n",
    "    build_edges_from_parquet(TABLE_PATHS['edge_rek_credit'], src_col='src', dst_col='dst',\n",
    "                             src_map=txn_map, dst_map=rekening_map, out_parquet=out_edge_txn_to, month=None)\n",
    "\n",
    "    # 3) Build simple aggregated features per pekerja (streaming)\n",
    "    print('Step 3: aggregate per-pekerja features (simple)')\n",
    "    # We need a mapping from nasabah -> pekerja; load the edge file we created\n",
    "    df_pekerja_edge = pd.read_parquet(out_edge_pekerja)\n",
    "    # build nasabah->pekerja map (pick first pekerja if multiple)\n",
    "    nasabah_to_pekerja = {}\n",
    "    df_pekerja_edge = df_pekerja_edge[df_pekerja_edge['src_nid'] >= 0]\n",
    "    for _, r in df_pekerja_edge.iterrows():\n",
    "        nasabah_to_pekerja[r['src_nid']] = int(r['dst_nid'])\n",
    "    # account->nasabah map: we can build from edge_nasabah_memiliki_simp\n",
    "    df_nasabah_simp = pd.read_parquet(out_edge_nasabah_simp)\n",
    "    acct_to_nasabah = {}\n",
    "    for _, r in df_nasabah_simp.iterrows():\n",
    "        acct_to_nasabah[int(r['dst_nid'])] = int(r['src_nid'])\n",
    "\n",
    "    # call simple aggregator\n",
    "    pekerja_feat_parquet = os.path.join(FEATURE_DIR, 'pekerja_features.parquet')\n",
    "    agg_features_per_pekerja_simple(TABLE_PATHS['node_transaksi'], acct_to_nasabah, nasabah_map, nasabah_to_pekerja,\n",
    "                                    out_parquet=pekerja_feat_parquet, months=months_train+months_val+months_test)\n",
    "\n",
    "    # 4) build heterodata from files\n",
    "    print('Step 4: build HeteroData (loading pekerja features, edges)')\n",
    "    map_files = {'pekerja': pekerja_map_path, 'nasabah': nasabah_map_path, 'rekening': rekening_map_path, 'transaksi': txn_map_path}\n",
    "    node_feature_files = {'pekerja': pekerja_feat_parquet}\n",
    "    edge_parquet_files = {\n",
    "        ('nasabah','is_pekerja_of','pekerja'): out_edge_pekerja,\n",
    "        ('nasabah','has_simp','rekening'): out_edge_nasabah_simp,\n",
    "        ('transaksi','from_acct','rekening'): out_edge_txn_from,\n",
    "        ('transaksi','to_acct','rekening'): out_edge_txn_to,\n",
    "    }\n",
    "    data = build_heterodata(map_files, node_feature_files, edge_parquet_files)\n",
    "\n",
    "    # 5) prepare train/val/test indices for pekerja (time-split using label parquet)\n",
    "    print('Step 5: prepare train/val/test indices from label file')\n",
    "    labels = pd.read_parquet(label_parquet_path)\n",
    "    # expected label file columns: pn (pekerja id original), nid (mapped nid), is_fraud, period\n",
    "    if 'nid' not in labels.columns:\n",
    "        labels = labels.merge(pd.DataFrame(list(pekerja_map.items()), columns=['pn','nid']), on='pn', how='left')\n",
    "    # filter by months\n",
    "    train_mask = labels['period'].isin(months_train)\n",
    "    val_mask = labels['period'].isin(months_val)\n",
    "    test_mask = labels['period'].isin(months_test)\n",
    "    # ensure y aligns with data['pekerja'].y ordering (assume nid is 0..N-1)\n",
    "    labels_sorted = labels.sort_values('nid')\n",
    "\n",
    "    train_idx = torch.tensor(labels_sorted[train_mask]['nid'].values.astype('int64'))\n",
    "    val_idx = torch.tensor(labels_sorted[val_mask]['nid'].values.astype('int64'))\n",
    "    test_idx = torch.tensor(labels_sorted[test_mask]['nid'].values.astype('int64'))\n",
    "\n",
    "    pos_idx = torch.tensor(labels_sorted[train_mask & (labels_sorted['is_fraud']==1)]['nid'].values.astype('int64'))\n",
    "\n",
    "    # 6) loaders\n",
    "    train_loader = make_neighbor_loader(data, train_idx, pos_idx, batch_size=KNOBS['batch_size_nodes'],\n",
    "                                       num_neighbors=KNOBS['num_neighbors'], oversample_ratio=4)\n",
    "    val_loader = make_neighbor_loader(data, val_idx, pos_idx=torch.tensor([]), batch_size=KNOBS['batch_size_nodes'],\n",
    "                                     num_neighbors=KNOBS['num_neighbors'], oversample_ratio=1, shuffle=False)\n",
    "    test_loader = make_neighbor_loader(data, test_idx, pos_idx=torch.tensor([]), batch_size=KNOBS['batch_size_nodes'],\n",
    "                                      num_neighbors=KNOBS['num_neighbors'], oversample_ratio=1, shuffle=False)\n",
    "\n",
    "    # 7) model\n",
    "    print('Step 6: create model and train')\n",
    "    # determine in_dims per node type\n",
    "    in_dims = {}\n",
    "    for ntype in data.node_types:\n",
    "        if hasattr(data[ntype], 'x'):\n",
    "            in_dims[ntype] = data[ntype].x.size(1)\n",
    "    metadata = (list(data.node_types), list(data.edge_types))\n",
    "    model = CompactHeteroSAGE(metadata=metadata, in_dims=in_dims, hidden=KNOBS['hidden_dim'], num_layers=2)\n",
    "\n",
    "    # 8) train\n",
    "    trained = train_model(model, train_loader, val_loader, device=KNOBS['device'], epochs=5, accum_steps=2)\n",
    "\n",
    "    # 9) final evaluation on test set\n",
    "    test_metrics = evaluate_model(trained, test_loader, device=KNOBS['device'])\n",
    "    print('Test metrics:', test_metrics)\n",
    "\n",
    "    # save model\n",
    "    torch.save(trained.state_dict(), os.path.join(\"D:\\zman\\graph\\notebook\\model\", 'hetero_sage_pekerja.pt'))\n",
    "    print('Version A pipeline completed. Artifacts saved under', \"D:\\zman\\graph\\notebook\\model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79c75244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: building id maps (streaming)\n",
      "Loading map from ./maps\\map_pekerja.pkl\n",
      "Loading map from ./maps\\map_nasabah.pkl\n",
      "Loading map from ./maps\\map_rekening.pkl\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# label_parquet_path must contain columns: pn, is_fraud, period. If nid not provided, script maps pn->nid via pekerja_map\u001b[39;00m\n\u001b[0;32m     10\u001b[0m label_parquet_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(BASE_DIR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpekerja_labels.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m \u001b[43mversion_a_runner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonths_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonths_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmonths_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_parquet_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 44\u001b[0m, in \u001b[0;36mversion_a_runner\u001b[1;34m(months_train, months_val, months_test, label_parquet_path, account_to_nasabah_map_parquet)\u001b[0m\n\u001b[0;32m     42\u001b[0m txn_iter \u001b[38;5;241m=\u001b[39m stream_table(TABLE_PATHS[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnode_transaksi\u001b[39m\u001b[38;5;124m'\u001b[39m], columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid_trx\u001b[39m\u001b[38;5;124m'\u001b[39m], month\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200000\u001b[39m)\n\u001b[0;32m     43\u001b[0m txn_map_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(MAP_DIR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmap_transaksi.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 44\u001b[0m txn_map \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_or_load_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtxn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid_trx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtxn_map_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# 2) Build edge files: map nasabah-pekerja, rekening->nasabah, transaksi->rekening\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStep 2: building edge parquet files (mapped integer ids)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 20\u001b[0m, in \u001b[0;36mbuild_or_load_map\u001b[1;34m(values_iter, key_col, map_path)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m vals:\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mapping:\n\u001b[1;32m---> 20\u001b[0m             \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mv\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m nid\n\u001b[0;32m     21\u001b[0m             nid \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(map_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# CLI / example usage\n",
    "# -----------------------------------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    # Example months split (edit according to your partitions e.g. '202308'..'202408')\n",
    "    months_train = ['202308','202309','202310','202311','202312','202401','202402','202403']\n",
    "    months_val = ['202404']\n",
    "    months_test = ['202405']\n",
    "    # label_parquet_path must contain columns: pn, is_fraud, period. If nid not provided, script maps pn->nid via pekerja_map\n",
    "    label_parquet_path = os.path.join(BASE_DIR, 'labels', 'pekerja_labels.parquet')\n",
    "    version_a_runner(months_train, months_val, months_test, label_parquet_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
