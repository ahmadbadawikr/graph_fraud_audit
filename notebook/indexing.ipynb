{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5b37a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import os\n",
    "import math\n",
    "\n",
    "BYTES_PER_TYPE = {\n",
    "    \"int8\": 1, \"int16\": 2, \"int32\": 4, \"int64\": 8,\n",
    "    \"uint8\": 1, \"uint16\": 2, \"uint32\": 4, \"uint64\": 8,\n",
    "    \"float\": 4, \"float16\": 2, \"float32\": 4, \"float64\": 8,\n",
    "    \"bool\": 1,\n",
    "}\n",
    "\n",
    "STRING_DEFAULT_BYTES = 16\n",
    "\n",
    "def patch_schema(schema):\n",
    "    fields = []\n",
    "    for field in schema:\n",
    "        if pa.types.is_decimal(field.type):\n",
    "            fields.append(pa.field(field.name, pa.string()))\n",
    "        else:\n",
    "            fields.append(field)\n",
    "    return pa.schema(fields)\n",
    "\n",
    "\n",
    "def estimate_array_size(arr, col_type):\n",
    "    n = len(arr)\n",
    "\n",
    "    if pa.types.is_integer(col_type) or pa.types.is_floating(col_type):\n",
    "        dtype = col_type.to_pandas_dtype()\n",
    "        return n * BYTES_PER_TYPE.get(dtype, 8)\n",
    "\n",
    "    if pa.types.is_boolean(col_type):\n",
    "        return n * 1\n",
    "\n",
    "    if pa.types.is_decimal(col_type):\n",
    "        return n * 16\n",
    "\n",
    "    if pa.types.is_string(col_type):\n",
    "        # Sample 5000\n",
    "        sample = arr.slice(0, min(5000, n)).to_pylist()\n",
    "        total_len = sum(len(x) for x in sample if x is not None)\n",
    "        non_null = sum(1 for x in sample if x is not None)\n",
    "        avg_len = (total_len / non_null) if non_null > 0 else STRING_DEFAULT_BYTES\n",
    "        return int(n * avg_len)\n",
    "\n",
    "    return n * 16\n",
    "\n",
    "\n",
    "# INPUT_DIR = \"D:\\\\zman\\\\graph\\\\data\"\n",
    "results = []\n",
    "files = [\n",
    "    'edge_pinj_credit',\n",
    "    'edge_pinj_debit',\n",
    "    'edge_simp_credit',\n",
    "    'edge_simp_debit',\n",
    "]\n",
    "\n",
    "# for file in os.listdir(INPUT_DIR)[:5]:\n",
    "for file in files:\n",
    "\n",
    "    if file.endswith(\".csv\"):\n",
    "        continue\n",
    "\n",
    "    path = os.path.join(\"/Users/ymnzaman/Documents/Project/Graph/data\", file)\n",
    "    # path = os.path.join(INPUT_DIR, file)\n",
    "\n",
    "\n",
    "    base = ds.dataset(path, format=\"parquet\", partitioning=\"hive\")\n",
    "    patched_schema = patch_schema(base.schema)\n",
    "    dataset = ds.dataset(path, format=\"parquet\", partitioning=\"hive\", schema=patched_schema)\n",
    "\n",
    "    print(f\"\\n=== Estimasi LMDB untuk: {file} ===\")\n",
    "\n",
    "    num_rows = dataset.count_rows()\n",
    "    print(\"Rows:\", num_rows)\n",
    "\n",
    "    total_bytes = 0\n",
    "\n",
    "    for col in dataset.schema.names:\n",
    "\n",
    "        col_type = dataset.schema.field(col).type\n",
    "        col_bytes = 0\n",
    "\n",
    "        # Scanner hanya untuk 1 kolom\n",
    "        scanner = dataset.scanner(columns=[col], batch_size=75_000)\n",
    "\n",
    "        for batch in scanner.to_batches():\n",
    "            arr = batch.column(0)\n",
    "            col_bytes += estimate_array_size(arr, col_type)\n",
    "\n",
    "        total_bytes += col_bytes\n",
    "\n",
    "        print(f\"{col:30} | {str(col_type):25} | {col_bytes/1024/1024:10.2f} MB\")\n",
    "\n",
    "    lmdb_bytes = int(total_bytes * 1.2)\n",
    "    gb = math.ceil(lmdb_bytes / (1024**3))\n",
    "\n",
    "    print(f\"\\nTOTAL ESTIMATED SIZE: {total_bytes/1024/1024/1024:.2f} GB\")\n",
    "    print(f\"RECOMMENDED LMDB MAP_SIZE: {gb} GB\\n\")\n",
    "\n",
    "    results.append((file, gb))\n",
    "\n",
    "print(\"\\n===================== RINGKASAN MAP_SIZE =====================\")\n",
    "for file, gb in results:\n",
    "    print(f\"{file:40} : {gb} GB\")\n",
    "print(\"==============================================================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b441bc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import lmdb\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "\n",
    "# =======================================================\n",
    "# MAP SIZE\n",
    "# =======================================================\n",
    "MAP_SIZE = {\n",
    "    # \"edge_nasabah_is_pekerja\": 1024 * 1024 * 1024 * 1,\n",
    "    # \"edge_nasabah_memiliki_pinj\": 1024 * 1024 * 1024 * 1,\n",
    "    # \"edge_nasabah_memiliki_simp\": 1024 * 1024 * 1024 * 15,\n",
    "    # \"edge_rek_credit\": 1024 * 1024 * 1024 * 2,\n",
    "    # \"edge_rek_debit\": 1024 * 1024 * 1024 * 2,\n",
    "    'edge_pinj_credit': 1024 * 1024 * 1024 *1,\n",
    "    'edge_pinj_debit': 1024 * 1024 * 1024 *1,\n",
    "    'edge_simp_credit': 1024 * 1024 * 1024 *1,\n",
    "    'edge_simp_debit': 1024 * 1024 * 1024 *1,\n",
    "}\n",
    "\n",
    "\n",
    "# =======================================================\n",
    "# PATCH SCHEMA DECIMAL → STRING\n",
    "# =======================================================\n",
    "def patch_schema(schema):\n",
    "    fields = []\n",
    "    for field in schema:\n",
    "        if pa.types.is_decimal(field.type):\n",
    "            fields.append(pa.field(field.name, pa.string()))\n",
    "        else:\n",
    "            fields.append(field)\n",
    "    return pa.schema(fields)\n",
    "\n",
    "\n",
    "# =======================================================\n",
    "# NORMALIZE KEY\n",
    "# =======================================================\n",
    "def normalize_key(v):\n",
    "    if v is None:\n",
    "        return None\n",
    "    v = str(v).strip()\n",
    "\n",
    "    if v.endswith(\".0\"):\n",
    "        v = v[:-2]\n",
    "\n",
    "    if v.startswith(\"0\") and len(v) > 1:\n",
    "        v = v.lstrip(\"0\")\n",
    "\n",
    "    return v\n",
    "\n",
    "\n",
    "# =======================================================\n",
    "# LMDB LOOKUP\n",
    "# =======================================================\n",
    "def lmdb_lookup(env, key):\n",
    "    key = normalize_key(key)\n",
    "    if key is None:\n",
    "        return None\n",
    "    with env.begin() as txn:\n",
    "        v = txn.get(key.encode())\n",
    "        return None if v is None else int(v.decode())\n",
    "\n",
    "\n",
    "def multi_lmdb_lookup(env_list, key):\n",
    "    key = normalize_key(key)\n",
    "    if key is None:\n",
    "        return None\n",
    "    key_b = key.encode()\n",
    "\n",
    "    for env in env_list:\n",
    "        with env.begin() as txn:\n",
    "            v = txn.get(key_b)\n",
    "            if v is not None:\n",
    "                return int(v.decode())\n",
    "    return None\n",
    "\n",
    "\n",
    "# =======================================================\n",
    "# MAIN INDEX FUNCTION\n",
    "# =======================================================\n",
    "def index_edges_dataset(edge_type, edge_folder, map_src, map_dst, out_lmdb, map_size):\n",
    "\n",
    "    # Source LMDB (bisa list)\n",
    "    if isinstance(map_src, list):\n",
    "        src_envs = [lmdb.open(x, readonly=True, lock=False) for x in map_src]\n",
    "        lookup_src = lambda k: multi_lmdb_lookup(src_envs, k)\n",
    "    else:\n",
    "        env_src = lmdb.open(map_src, readonly=True, lock=False)\n",
    "        lookup_src = lambda k: lmdb_lookup(env_src, k)\n",
    "\n",
    "    # Target LMDB (bisa list)\n",
    "    if isinstance(map_dst, list):\n",
    "        dst_envs = [lmdb.open(x, readonly=True, lock=False) for x in map_dst]\n",
    "        lookup_dst = lambda k: multi_lmdb_lookup(dst_envs, k)\n",
    "    else:\n",
    "        env_dst = lmdb.open(map_dst, readonly=True, lock=False)\n",
    "        lookup_dst = lambda k: lmdb_lookup(env_dst, k)\n",
    "\n",
    "    # Output LMDB\n",
    "    env_out = lmdb.open(out_lmdb, map_size=map_size)\n",
    "\n",
    "    # Dataset\n",
    "    base = ds.dataset(edge_folder, format=\"parquet\", partitioning=\"hive\")\n",
    "    patched_schema = patch_schema(base.schema)\n",
    "    dataset = ds.dataset(edge_folder, format=\"parquet\", partitioning=\"hive\", schema=patched_schema)\n",
    "\n",
    "    scanner = dataset.scanner(columns=[\"src\", \"dst\"])\n",
    "\n",
    "    print(f\"\\n=== Start indexing {edge_type} ===\")\n",
    "    edge_id = 0\n",
    "\n",
    "    with env_out.begin(write=True) as txn:\n",
    "        for batch in scanner.to_batches():\n",
    "            d = batch.to_pydict()\n",
    "\n",
    "            for s, t in zip(d[\"src\"], d[\"dst\"]):\n",
    "\n",
    "                sid = lookup_src(s)\n",
    "                tid = lookup_dst(t)\n",
    "\n",
    "                if sid is None or tid is None:\n",
    "                    if DEBUG:\n",
    "                        print(f\"[MISS] src={s} sid={sid} | dst={t} tid={tid}\")\n",
    "                    continue\n",
    "\n",
    "                txn.put(str(edge_id).encode(), f\"{sid},{tid}\".encode())\n",
    "                edge_id += 1\n",
    "\n",
    "            print(f\"{edge_type}: {edge_id:,} edges...\", end=\"\\r\")\n",
    "\n",
    "    print(f\"\\n✓ Done {edge_type}: {edge_id:,} edges.\")\n",
    "    print(f\"Saved → {out_lmdb}\")\n",
    "\n",
    "\n",
    "# =======================================================\n",
    "# CONFIG BENAR (src/dst bisa list atau string)\n",
    "# =======================================================\n",
    "edges_config = {\n",
    "    # \"edge_rek_credit\": {\n",
    "    #     \"src\": \"transaksi.lmdb\",\n",
    "    #     \"dst\": [\"simpanan.lmdb\", \"pinjaman.lmdb\"]\n",
    "    # },\n",
    "    # \"edge_rek_debit\": {\n",
    "    #     \"src\": [\"simpanan.lmdb\", \"pinjaman.lmdb\"],\n",
    "    #     \"dst\": \"transaksi.lmdb\"\n",
    "    # }\n",
    "    \"edge_pinj_credit\": {\n",
    "        \"src\": \"transaksi.lmdb\",\n",
    "        \"dst\": \"pinjaman.lmdb\"\n",
    "    },\n",
    "    \"edge_pinj_debit\": {\n",
    "        \"src\": \"pinjaman.lmdb\",\n",
    "        \"dst\": \"transaksi.lmdb\"\n",
    "    },\n",
    "    \"edge_simp_credit\": {\n",
    "        \"src\": \"transaksi.lmdb\",\n",
    "        \"dst\": \"simpanan.lmdb\"\n",
    "    },\n",
    "    \"edge_simp_debit\": {\n",
    "        \"src\": \"simpanan.lmdb\",\n",
    "        \"dst\": \"transaksi.lmdb\"\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "root_edges = \"/Users/ymnzaman/Documents/Project/Graph/data\"\n",
    "root_maps  = \"/Users/ymnzaman/Documents/Project/Graph/lmdb_node_mapping\"\n",
    "out_root   = \"/Users/ymnzaman/Documents/Project/Graph/lmdb_edge_indexing\"\n",
    "os.makedirs(out_root, exist_ok=True)\n",
    "\n",
    "\n",
    "# =======================================================\n",
    "# MAIN LOOP (FULLY FIXED)\n",
    "# =======================================================\n",
    "for edge_type, cfg in edges_config.items():\n",
    "\n",
    "    folder = os.path.join(root_edges, edge_type)\n",
    "\n",
    "    # src bisa list\n",
    "    if isinstance(cfg[\"src\"], list):\n",
    "        src_map = [os.path.join(root_maps, x) for x in cfg[\"src\"]]\n",
    "    else:\n",
    "        src_map = os.path.join(root_maps, cfg[\"src\"])\n",
    "\n",
    "    # dst bisa list\n",
    "    if isinstance(cfg[\"dst\"], list):\n",
    "        dst_map = [os.path.join(root_maps, x) for x in cfg[\"dst\"]]\n",
    "    else:\n",
    "        dst_map = os.path.join(root_maps, cfg[\"dst\"])\n",
    "\n",
    "    out_lmdb_path = os.path.join(out_root, f\"{edge_type}.lmdb\")\n",
    "\n",
    "    index_edges_dataset(\n",
    "        edge_type=edge_type,\n",
    "        edge_folder=folder,\n",
    "        map_src=src_map,\n",
    "        map_dst=dst_map,\n",
    "        out_lmdb=out_lmdb_path,\n",
    "        map_size=MAP_SIZE[edge_type]\n",
    "    )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf1535b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import lmdb\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow as pa\n",
    "\n",
    "# ---------- config ----------\n",
    "ROOT_EDGES = r\"D:\\\\zman\\\\graph\\\\data\"\n",
    "ROOT_MAPS  = r\"D:\\\\zman\\\\graph\\\\notebook\\\\lmdb_node_mapping\"\n",
    "OUT_ROOT   = r\"D:\\\\zman\\\\graph\\\\notebook\\\\lmdb_edge_indexing\"\n",
    "os.makedirs(OUT_ROOT, exist_ok=True)\n",
    "\n",
    "MAP_SIZE = 1024 * 1024 * 1024 * 2\n",
    "DEBUG = True   # set False to silence per-row MISS prints\n",
    "SAMPLE_MISS_LIMIT = 10\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def patch_schema(schema):\n",
    "    fields = []\n",
    "    for field in schema:\n",
    "        if pa.types.is_decimal(field.type):\n",
    "            fields.append(pa.field(field.name, pa.string()))\n",
    "        else:\n",
    "            fields.append(field)\n",
    "    return pa.schema(fields)\n",
    "\n",
    "def normalize_key(v):\n",
    "    if v is None:\n",
    "        return None\n",
    "    s = str(v).strip()\n",
    "    if s.endswith(\".0\"):\n",
    "        s = s[:-2]\n",
    "    # don't aggressively strip leading zeros here unless you know LMDB keys don't have them\n",
    "    return s\n",
    "\n",
    "def lmdb_open(path):\n",
    "    return lmdb.open(path, readonly=True, lock=False)\n",
    "\n",
    "def lmdb_get_single(env, key):\n",
    "    if key is None:\n",
    "        return None\n",
    "    k = normalize_key(key).encode()\n",
    "    with env.begin() as txn:\n",
    "        v = txn.get(k)\n",
    "        return None if v is None else v.decode()\n",
    "\n",
    "def lmdb_get_multi(envs, key):\n",
    "    if key is None:\n",
    "        return None\n",
    "    k = normalize_key(key).encode()\n",
    "    for env in envs:\n",
    "        with env.begin() as txn:\n",
    "            v = txn.get(k)\n",
    "            if v is not None:\n",
    "                return v.decode()\n",
    "    return None\n",
    "\n",
    "# ---------- indexer generic ----------\n",
    "def index_edge(\n",
    "    edge_type,\n",
    "    edge_folder,\n",
    "    src_map,    # either path or list of paths\n",
    "    dst_map,    # either path or list of paths\n",
    "    out_lmdb_path,\n",
    "    map_size=MAP_SIZE,\n",
    "    src_col_name=\"src\",\n",
    "    dst_col_name=\"dst\"\n",
    "):\n",
    "    print(f\"\\n=== INDEX {edge_type} ===\")\n",
    "    # open maps (single or list)\n",
    "    if isinstance(src_map, list):\n",
    "        src_envs = [lmdb_open(p) for p in src_map]\n",
    "        lookup_src = lambda k: lmdb_get_multi(src_envs, k)\n",
    "    else:\n",
    "        env_src = lmdb_open(src_map)\n",
    "        lookup_src = lambda k: lmdb_get_single(env_src, k)\n",
    "\n",
    "    if isinstance(dst_map, list):\n",
    "        dst_envs = [lmdb_open(p) for p in dst_map]\n",
    "        lookup_dst = lambda k: lmdb_get_multi(dst_envs, k)\n",
    "    else:\n",
    "        env_dst = lmdb_open(dst_map)\n",
    "        lookup_dst = lambda k: lmdb_get_single(env_dst, k)\n",
    "\n",
    "    # prepare dataset\n",
    "    base = ds.dataset(edge_folder, format=\"parquet\", partitioning=\"hive\")\n",
    "    patched_schema = patch_schema(base.schema)\n",
    "    dataset = ds.dataset(edge_folder, format=\"parquet\", partitioning=\"hive\", schema=patched_schema)\n",
    "    scanner = dataset.scanner(columns=[src_col_name, dst_col_name])\n",
    "\n",
    "    # stats\n",
    "    total = 0\n",
    "    matched = 0\n",
    "    miss_src = 0\n",
    "    miss_dst = 0\n",
    "    miss_both = 0\n",
    "    miss_samples = []\n",
    "\n",
    "    # open output LMDB writer\n",
    "    env_out = lmdb.open(out_lmdb_path, map_size=map_size)\n",
    "    edge_id = 0\n",
    "\n",
    "    with env_out.begin(write=True) as txn:\n",
    "        for batch in scanner.to_batches():\n",
    "            cols = batch.to_pydict()\n",
    "            srcs = cols.get(src_col_name, [])\n",
    "            dsts = cols.get(dst_col_name, [])\n",
    "\n",
    "            for s_raw, d_raw in zip(srcs, dsts):\n",
    "                total += 1\n",
    "                s_key = normalize_key(s_raw)\n",
    "                d_key = normalize_key(d_raw)\n",
    "\n",
    "                sid = lookup_src(s_key)\n",
    "                did = lookup_dst(d_key)\n",
    "\n",
    "                # sid / did are strings read from LMDB; treat None as missing\n",
    "                if sid is None and did is None:\n",
    "                    miss_both += 1\n",
    "                    if len(miss_samples) < SAMPLE_MISS_LIMIT:\n",
    "                        miss_samples.append((\"both\", s_raw, d_raw, s_key, d_key))\n",
    "                    if DEBUG:\n",
    "                        print(f\"[MISS BOTH] src={s_raw} -> {s_key} | dst={d_raw} -> {d_key}\")\n",
    "                    continue\n",
    "                if sid is None:\n",
    "                    miss_src += 1\n",
    "                    if len(miss_samples) < SAMPLE_MISS_LIMIT:\n",
    "                        miss_samples.append((\"src\", s_raw, d_raw, s_key, d_key))\n",
    "                    if DEBUG:\n",
    "                        print(f\"[MISS SRC] src={s_raw} -> {s_key} | dst={d_raw} -> {d_key} (did ok)\")\n",
    "                    continue\n",
    "                if did is None:\n",
    "                    miss_dst += 1\n",
    "                    if len(miss_samples) < SAMPLE_MISS_LIMIT:\n",
    "                        miss_samples.append((\"dst\", s_raw, d_raw, s_key, d_key))\n",
    "                    if DEBUG:\n",
    "                        print(f\"[MISS DST] src={s_raw} -> {s_key} (sid ok) | dst={d_raw} -> {d_key}\")\n",
    "                    continue\n",
    "\n",
    "                # write edge as \"sid,did\" (we keep them as strings; you can convert to int if needed)\n",
    "                txn.put(str(edge_id).encode(), f\"{sid},{did}\".encode())\n",
    "                edge_id += 1\n",
    "                matched += 1\n",
    "\n",
    "            # progress\n",
    "            if total % 100000 == 0:\n",
    "                print(f\"{edge_type}: processed {total:,} rows, matched {matched:,}\")\n",
    "\n",
    "    print(\"\\n--- summary ---\")\n",
    "    print(f\"total rows scanned   : {total:,}\")\n",
    "    print(f\"matched edges written : {matched:,}\")\n",
    "    print(f\"miss src only         : {miss_src:,}\")\n",
    "    print(f\"miss dst only         : {miss_dst:,}\")\n",
    "    print(f\"miss both             : {miss_both:,}\")\n",
    "    print(f\"out LMDB path         : {out_lmdb_path}\")\n",
    "    if miss_samples:\n",
    "        print(\"\\nEXAMPLE MISS SAMPLES (type, raw_src, raw_dst, norm_src, norm_dst):\")\n",
    "        for ex in miss_samples:\n",
    "            print(\" \", ex)\n",
    "    print(\"=== done ===\\n\")\n",
    "\n",
    "\n",
    "# ---------- run for credit and debit with correct src/dst ----------\n",
    "# credit: src = transaksi, dst = simpanan|pinjaman\n",
    "index_edge(\n",
    "    edge_type=\"edge_rek_credit\",\n",
    "    edge_folder=os.path.join(ROOT_EDGES, \"edge_rek_credit\"),\n",
    "    src_map=os.path.join(ROOT_MAPS, \"transaksi.lmdb\"),\n",
    "    dst_map=[os.path.join(ROOT_MAPS, \"simpanan.lmdb\"), os.path.join(ROOT_MAPS, \"pinjaman.lmdb\")],\n",
    "    out_lmdb_path=os.path.join(OUT_ROOT, \"edge_rek_credit.lmdb\"),\n",
    "    src_col_name=\"src\",\n",
    "    dst_col_name=\"dst\"\n",
    ")\n",
    "\n",
    "# debit: src = simpanan|pinjaman, dst = transaksi\n",
    "index_edge(\n",
    "    edge_type=\"edge_rek_debit\",\n",
    "    edge_folder=os.path.join(ROOT_EDGES, \"edge_rek_debit\"),\n",
    "    src_map=[os.path.join(ROOT_MAPS, \"simpanan.lmdb\"), os.path.join(ROOT_MAPS, \"pinjaman.lmdb\")],\n",
    "    dst_map=os.path.join(ROOT_MAPS, \"transaksi.lmdb\"),\n",
    "    out_lmdb_path=os.path.join(OUT_ROOT, \"edge_rek_debit.lmdb\"),\n",
    "    src_col_name=\"src\",\n",
    "    dst_col_name=\"dst\"\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55e9b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmdb\n",
    "\n",
    "def validate_edge_lmdb(lmdb_path, sample_size=5):\n",
    "    print(\"=\"*50)\n",
    "    print(f\"LMDB Path             : {lmdb_path}\")\n",
    "    print(\"-\"*50)\n",
    "\n",
    "    env = lmdb.open(lmdb_path, readonly=True, lock=False)\n",
    "\n",
    "    total_keys = 0\n",
    "    none_values = 0\n",
    "    bad_format = 0\n",
    "    samples = []\n",
    "\n",
    "    with env.begin() as txn:\n",
    "        cursor = txn.cursor()\n",
    "\n",
    "        for k, v in cursor:\n",
    "            total_keys += 1\n",
    "            val = v.decode()\n",
    "\n",
    "            if val.strip() == \"\" or val is None:\n",
    "                none_values += 1\n",
    "                continue\n",
    "\n",
    "            # Format harus \"sid,did\"\n",
    "            if \",\" not in val:\n",
    "                bad_format += 1\n",
    "                continue\n",
    "\n",
    "            if len(samples) < sample_size:\n",
    "                samples.append((k.decode(), val))\n",
    "\n",
    "    print(f\"Total Unique Keys     : {total_keys:,}\")\n",
    "    print(f\"Nilai 'None' / Empty  : {none_values:,}\")\n",
    "    print(f\"Bad Format (no comma) : {bad_format:,}\")\n",
    "    print(\"-\"*50)\n",
    "    print(\"Contoh Key-Value:\")\n",
    "    for item in samples:\n",
    "        print(\"  \", item)\n",
    "\n",
    "    print(\"=\"*50)\n",
    "\n",
    "\n",
    "paths = [\n",
    "    \"edge_rek_credit.lmdb\",\n",
    "    \"edge_rek_debit.lmdb\",\n",
    "    # \"edge_nasabah_is_pekerja.lmdb\",\n",
    "    # \"edge_nasabah_memiliki_simp.lmdb\",\n",
    "    # \"edge_nasabah_memiliki_pinj.lmdb\"\n",
    "    'edge_pinj_credit.lmdb',\n",
    "    'edge_pinj_debit.lmdb',\n",
    "    'edge_simp_credit.lmdb',\n",
    "    'edge_simp_debit.lmdb',\n",
    "]\n",
    "\n",
    "for p in paths:\n",
    "    validate_edge_lmdb(f\"/Users/ymnzaman/Documents/Project/Graph/lmdb_edge_indexing/{p}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6988307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as ds\n",
    "\n",
    "# Edge\n",
    "edge = ds.dataset(\"D:\\zman\\graph\\data/edge_rek_credit\")\n",
    "print(\"edge_rek_credit\")\n",
    "print(edge.schema, end=\"\\n\\n\")\n",
    "\n",
    "edge_ = ds.dataset(\"D:\\zman\\graph\\data/edge_rek_debit\")\n",
    "print(\"edge_rek_debit\")\n",
    "print(edge_.schema, end=\"\\n\\n\")\n",
    "\n",
    "# Node transaksi\n",
    "trx = ds.dataset(\"D:\\zman\\graph\\data/node_transaksi\")\n",
    "print(\"node_transaksi\")\n",
    "print(trx.schema)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bc1ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scanner = edge.scanner()\n",
    "batch = scanner.head(5)\n",
    "batch.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5d6bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "scanner_ = edge_.scanner()\n",
    "batch_ = scanner_.head(5)\n",
    "batch_.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390763bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scanner_trx = trx.scanner()\n",
    "batch_trx = scanner_trx.head(5)\n",
    "batch_trx.to_pandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
